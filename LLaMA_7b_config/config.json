{
	"hidden_size": 4096,
	"n_layer": 32,
	"n_head": 32,
	"vocab_size": 32000,
	"multiple_of": 256,
	"norm_eps": 1e-5,
	"pad_token_id": -1,
	"bos_token_id":1,
	"eos_token_id":2,
	"max_seq_len": 2048,
	"inference": false,
	"max_batch_size": 32
}